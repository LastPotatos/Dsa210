{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83eddcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (0.2.66)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from yfinance) (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from yfinance) (2.3.5)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from yfinance) (2.32.5)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from yfinance) (0.0.12)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from yfinance) (4.5.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from yfinance) (2.4.7)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from yfinance) (3.18.3)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from yfinance) (4.14.2)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from yfinance) (0.13.0)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from yfinance) (6.33.1)\n",
      "Requirement already satisfied: websockets>=13.0 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.15.0)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2025.11.12)\n",
      "Requirement already satisfied: pycparser in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.23)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from requests>=2.31->yfinance) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from requests>=2.31->yfinance) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from requests>=2.31->yfinance) (2.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\han aga\\desktop\\dsa 210 anime\\python.venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance\n",
    "!pip install tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a40284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39 tickers to process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:   0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Toei Animation (TYO:4816) -> yfinance ticker: 4816.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:   3%|▎         | 1/39 [00:03<02:17,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing IG Port (Production I.G (Wit Studio, Signal.MD), TYO:3791) -> yfinance ticker: 3791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: 3791\"}}}\n",
      "$3791: possibly delisted; no timezone found\n",
      "Tickers:   5%|▌         | 2/39 [00:05<01:44,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WARNING: no historical data for 3791. Skipping.\n",
      "\n",
      "Processing Bandai Namco Holdings (TYO:7832) -> yfinance ticker: 7832.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:   8%|▊         | 3/39 [00:08<01:35,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Sony Group (Aniplex (Crunchyroll), TYO:6758) -> yfinance ticker: 6758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: 6758\"}}}\n",
      "$6758: possibly delisted; no timezone found\n",
      "Tickers:  10%|█         | 4/39 [00:10<01:27,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WARNING: no historical data for 6758. Skipping.\n",
      "\n",
      "Processing Kadokawa Corporation (TYO:9468) -> yfinance ticker: 9468.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  13%|█▎        | 5/39 [00:13<01:24,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Avex Group (Anime Music / Production) (TYO:7860) -> yfinance ticker: 7860.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  15%|█▌        | 6/39 [00:15<01:18,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Bushiroad (Media Mix / Anime IP) (TYO:7815) -> yfinance ticker: 7815.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  18%|█▊        | 7/39 [00:17<01:12,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing CyberAgent (Owner of Abema (supports anime), TYO:4751) -> yfinance ticker: 4751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: 4751\"}}}\n",
      "$4751: possibly delisted; no timezone found\n",
      "Tickers:  21%|██        | 8/39 [00:19<01:08,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WARNING: no historical data for 4751. Skipping.\n",
      "\n",
      "Processing TV Tokyo Holdings (TYO:9413) -> yfinance ticker: 9413.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  23%|██▎       | 9/39 [00:21<01:05,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Fuji Media Holdings (TYO:4676) -> yfinance ticker: 4676.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  26%|██▌       | 10/39 [00:23<01:03,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Nippon TV Holdings (Owns Studio Ghibli) (TYO:9404) -> yfinance ticker: 9404.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  28%|██▊       | 11/39 [00:25<01:02,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Tokyo Broadcasting System (TBS) (TYO:9401) -> yfinance ticker: 9401.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  31%|███       | 12/39 [00:28<01:01,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Asahi Broadcasting Group (TYO:9405) -> yfinance ticker: 9405.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  33%|███▎      | 13/39 [00:30<00:59,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing WOWOW (Anime channel) (TYO:4839) -> yfinance ticker: 4839.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  36%|███▌      | 14/39 [00:33<00:59,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Toho Co. (Ltd., TYO:9602) -> yfinance ticker: 9602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: 9602\"}}}\n",
      "$9602: possibly delisted; no timezone found\n",
      "Tickers:  38%|███▊      | 15/39 [00:35<00:54,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WARNING: no historical data for 9602. Skipping.\n",
      "\n",
      "Processing Shochiku Co. (Ltd., TYO:9601) -> yfinance ticker: 9601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: 9601\"}}}\n",
      "$9601: possibly delisted; no timezone found\n",
      "Tickers:  41%|████      | 16/39 [00:37<00:51,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WARNING: no historical data for 9601. Skipping.\n",
      "\n",
      "Processing Kadokawa Pictures (covered by Kadokawa) (TYO:9468) -> yfinance ticker: 9468.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  44%|████▎     | 17/39 [00:39<00:49,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Happinet (anime distributor) (TYO:7552) -> yfinance ticker: 7552.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  46%|████▌     | 18/39 [00:41<00:46,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Shueisha (part of Hitotsubashi Group (not public)) -> yfinance ticker: not public)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: NOT PUBLIC)\"}}}\n",
      "$NOT PUBLIC): possibly delisted; no timezone found\n",
      "Tickers:  49%|████▊     | 19/39 [00:43<00:44,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WARNING: no historical data for not public). Skipping.\n",
      "\n",
      "Processing Square Enix Holdings (TYO:9684) -> yfinance ticker: 9684.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  51%|█████▏    | 20/39 [00:46<00:42,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Media Do (digital manga) (TYO:3678) -> yfinance ticker: 3678.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  54%|█████▍    | 21/39 [00:48<00:38,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Takara Tomy (TYO:7867) -> yfinance ticker: 7867.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  56%|█████▋    | 22/39 [00:50<00:37,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Sanrio (Hello Kitty (anime collabs), TYO:8136) -> yfinance ticker: 8136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$8136: possibly delisted; no timezone found\n",
      "Tickers:  59%|█████▉    | 23/39 [00:52<00:32,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WARNING: no historical data for 8136. Skipping.\n",
      "\n",
      "Processing Broccoli Co. (Merch & anime goods) (TYO:2706) -> yfinance ticker: 2706.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$2706.T: possibly delisted; no timezone found\n",
      "Tickers:  62%|██████▏   | 24/39 [00:54<00:29,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WARNING: no historical data for 2706.T. Skipping.\n",
      "\n",
      "Processing King Records (Starchild) (Owned by Kodansha (not public)) -> yfinance ticker: not public)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: NOT PUBLIC)\"}}}\n",
      "$NOT PUBLIC): possibly delisted; no timezone found\n",
      "Tickers:  64%|██████▍   | 25/39 [00:55<00:25,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WARNING: no historical data for not public). Skipping.\n",
      "\n",
      "Processing Pony Canyon (TYO:4763) -> yfinance ticker: 4763.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  67%|██████▋   | 26/39 [00:57<00:24,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Netflix (Anime Production) (NASDAQ:NFLX) -> yfinance ticker: NFLX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  69%|██████▉   | 27/39 [00:59<00:22,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Amazon (Anime Licensing) (NASDAQ:AMZN) -> yfinance ticker: AMZN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  72%|███████▏  | 28/39 [01:01<00:20,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Disney (Anime streaming) (NYSE:DIS) -> yfinance ticker: DIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  74%|███████▍  | 29/39 [01:03<00:20,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Rakuten (Rakuten TV) (TYO:4755) -> yfinance ticker: 4755.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  77%|███████▋  | 30/39 [01:05<00:18,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Sega Sammy Holdings (TYO:6460) -> yfinance ticker: 6460.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  79%|███████▉  | 31/39 [01:07<00:16,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Konami Group (TYO:9766) -> yfinance ticker: 9766.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  82%|████████▏ | 32/39 [01:10<00:14,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Capcom (TYO:9697) -> yfinance ticker: 9697.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  85%|████████▍ | 33/39 [01:12<00:12,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Koei Tecmo (TYO:3635) -> yfinance ticker: 3635.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  87%|████████▋ | 34/39 [01:14<00:11,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing NTT Docomo (Anime collaborations) (TYO:9437) -> yfinance ticker: 9437.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$9437.T: possibly delisted; no timezone found\n",
      "Tickers:  90%|████████▉ | 35/39 [01:16<00:08,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WARNING: no historical data for 9437.T. Skipping.\n",
      "\n",
      "Processing SoftBank Group (TYO:9984) -> yfinance ticker: 9984.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  92%|█████████▏| 36/39 [01:18<00:06,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing LINE Yahoo Japan (Z Holdings) (TYO:4689) -> yfinance ticker: 4689.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  95%|█████████▍| 37/39 [01:21<00:04,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing GREE (anime mobile games) (TYO:3632) -> yfinance ticker: 3632.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers:  97%|█████████▋| 38/39 [01:23<00:02,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing DeNA (anime mobile games) (TYO:2432) -> yfinance ticker: 2432.T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tickers: 100%|██████████| 39/39 [01:25<00:00,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved CSV: yearly_values.csv\n",
      "Saved XML: yearly_values.xml\n",
      "\n",
      "Done. Summary:\n",
      "  Companies processed: 29\n",
      "  Failed companies: 10 (see printed warnings above)\n",
      "  Output files: yearly_values.csv, yearly_values.xml\n",
      "\n",
      "Notes:\n",
      " - Values converted to USD when possible using Yahoo FX tickers (e.g., 'JPY=X').\n",
      " - If sharesOutstanding was missing for a company, the script used Close price as a proxy (not a true market cap).\n",
      " - Mixed-currency companies may not be included in TotalValue_USD if FX conversion failed.\n",
      "\n",
      "Please check the CSV for per-company currency/unit issues before using totals for analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Google Colab ready: Multi-studio historical market-cap aggregator by YEAR (CSV + XML)\n",
    "# - Reads studios.txt (format: \"Name, TICKER\" per line; supports TYO:, NASDAQ:, NYSE:)\n",
    "# - Uses yfinance to fetch history and sharesOutstanding\n",
    "# - Computes daily market cap (Close * sharesOutstanding) when possible\n",
    "# - Attempts to convert to USD using FX tickers like \"JPY=X\" or \"<CUR>USD=X\"\n",
    "# - Aggregates by YEAR using last trading-day of each year (gives year-end market cap)\n",
    "# - Outputs yearly_values.csv and yearly_values.xml\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "# -----------------------\n",
    "# Helper functions\n",
    "# -----------------------\n",
    "def normalize_ticker(raw_ticker):\n",
    "    \"\"\"Normalize user-style tickers to yfinance/Yahoo format.\n",
    "       Accept examples: 'TYO:4816', 'NASDAQ:NFLX', 'NYSE:DIS', '4816.T', 'NFLX'\n",
    "    \"\"\"\n",
    "    raw = raw_ticker.strip()\n",
    "    if ':' in raw:\n",
    "        prefix, code = raw.split(':', 1)\n",
    "        prefix = prefix.strip().upper()\n",
    "        code = code.strip()\n",
    "        if prefix in ('TYO', 'TSE', 'JPX', 'TOKYO'):\n",
    "            # Yahoo uses .T for Tokyo Exchange\n",
    "            return f\"{code}.T\"\n",
    "        elif prefix in ('NASDAQ', 'NAS'):\n",
    "            return code\n",
    "        elif prefix in ('NYSE',):\n",
    "            return code\n",
    "        elif prefix in ('OTC',):\n",
    "            return code  # why not\n",
    "        else:\n",
    "            # fallback: just return the code\n",
    "            return code\n",
    "    else:\n",
    "        # already maybe in Yahoo format; return raw\n",
    "        return raw\n",
    "\n",
    "def read_studios_file(path=\"studios.txt\"):\n",
    "    \"\"\"Reads studios.txt and returns list of (name, raw_ticker) for lines that have tickers.\"\"\"\n",
    "    entries = []\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"{path} not found. Upload your studios.txt to Colab filesystem.\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            # expect \"Name, TICKER\"\n",
    "            if ',' not in line:\n",
    "                # skip lines without a comma\n",
    "                continue\n",
    "            name, ticker = line.split(',', 1)\n",
    "            name = name.strip()\n",
    "            ticker = ticker.strip()\n",
    "            # ignore commented-in-line\n",
    "            if ticker.startswith('#') or ticker == '':\n",
    "                continue\n",
    "            entries.append((name, ticker))\n",
    "    return entries\n",
    "\n",
    "def attempt_fx_conversion_series(series_dates, from_currency):\n",
    "    \"\"\"Attempt to fetch FX series to convert given currency to USD.\n",
    "       Returns a pd.Series indexed by date with FX rate (units: 1 USD = rate in that currency) OR\n",
    "       returns series of USD-per-unit (i.e., multiplier to convert currency -> USD).\n",
    "       \n",
    "       Yahoo FX tickers examples:\n",
    "         USDJPY pair ticker on Yahoo: 'JPY=X' returns USD/JPY? Historically 'JPY=X' = USDJPY (i.e., 1 USD = X JPY)\n",
    "       We'll fetch <CUR>USD=X (e.g., 'JPYUSD=X') often fails, so strategy:\n",
    "         - If from_currency == 'JPY': use 'JPY=X' -> this gives USD/JPY (1 USD = X JPY) => to convert JPY -> USD, divide JPY_amount by (USD/JPY)\n",
    "         - For other currencies try '<CUR>=X' or '<CUR>USD=X' patterns; fallback = None\n",
    "    \"\"\"\n",
    "    cur = from_currency.upper()\n",
    "    # If already USD, return ones\n",
    "    if cur in ('USD', 'US$'):\n",
    "        return pd.Series(1.0, index=series_dates)\n",
    "    # Known mapping: for JPY use 'JPY=X' (Yahoo returns USD/JPY, value ~110 meaning 1 USD = 110 JPY)\n",
    "    # To convert JPY_amount to USD: USD = JPY_amount / (USD/JPY rate)\n",
    "    try_candidates = []\n",
    "    if cur == 'JPY':\n",
    "        try_candidates = ['JPY=X']\n",
    "    else:\n",
    "        # try EURUSD=X, GBPUSD=X, etc.\n",
    "        try_candidates = [f\"{cur}USD=X\", f\"{cur}=X\"]\n",
    "    for fx in try_candidates:\n",
    "        try:\n",
    "            ticker_fx = yf.Ticker(fx)\n",
    "            hist_fx = ticker_fx.history(period=\"max\", auto_adjust=False)\n",
    "            if hist_fx is None or hist_fx.empty:\n",
    "                continue\n",
    "            # use Close as FX rate\n",
    "            fx_close = hist_fx['Close'].rename('FX_Close')\n",
    "            # reindex to given series_dates with forward/backfill\n",
    "            fx_close = fx_close.reindex(series_dates.union(fx_close.index)).sort_index().ffill().reindex(series_dates)\n",
    "            return fx_close\n",
    "        except Exception as e:\n",
    "            # print(\"FX fetch fail for\", fx, e)\n",
    "            continue\n",
    "    # If we get here, failed to fetch FX\n",
    "    return None\n",
    "\n",
    "# -----------------------\n",
    "# Main processing\n",
    "# -----------------------\n",
    "\n",
    "def main():\n",
    "    # read studios.txt from working dir\n",
    "    entries = read_studios_file(\"studios.txt\")\n",
    "    if not entries:\n",
    "        print(\"No valid entries found in studios.txt. Please add lines like 'Toei Animation, TYO:4816'\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(entries)} tickers to process.\")\n",
    "    # build mapping of normalized ticker to (name, raw_ticker)\n",
    "    normalized = []\n",
    "    for name, raw_t in entries:\n",
    "        ytick = normalize_ticker(raw_t)\n",
    "        normalized.append((name, raw_t, ytick))\n",
    "    # iterate and fetch using yfinance\n",
    "    per_company_yearly = {}   # dict name -> DataFrame with index=year, columns=['market_cap_USD'/'market_cap_RAW']\n",
    "    company_currency = {}     # name -> currency string\n",
    "    failed_companies = []\n",
    "    all_years = set()\n",
    "\n",
    "    for (name, raw, ytick) in tqdm(normalized, desc=\"Tickers\"):\n",
    "        time.sleep(0.5)  # be polite\n",
    "        try:\n",
    "            print(f\"\\nProcessing {name} ({raw}) -> yfinance ticker: {ytick}\")\n",
    "            tk = yf.Ticker(ytick)\n",
    "            info = {}\n",
    "            try:\n",
    "                info = tk.info\n",
    "            except Exception:\n",
    "                # sometimes .info is rate-limited; fallback to light approach\n",
    "                info = {}\n",
    "            currency = info.get('currency', None)\n",
    "            shares_out = info.get('sharesOutstanding', None)  # may be None\n",
    "            # fetch history\n",
    "            hist = tk.history(period=\"max\", auto_adjust=False)  # keep raw close\n",
    "            if hist is None or hist.empty:\n",
    "                print(f\"  WARNING: no historical data for {ytick}. Skipping.\")\n",
    "                failed_companies.append((name, ytick, \"no-history\"))\n",
    "                continue\n",
    "            # ensure Date index is timezone-naive date index\n",
    "            hist = hist.sort_index()\n",
    "            # fix column names\n",
    "            if 'Close' not in hist.columns and 'close' in hist.columns:\n",
    "                hist['Close'] = hist['close']\n",
    "            # compute daily market cap if shares available\n",
    "            if shares_out and isinstance(shares_out, (int, float)) and shares_out > 0:\n",
    "                hist['MarketCap_RAW'] = hist['Close'] * shares_out\n",
    "                hist['MarketCap_raw_unit'] = currency or 'UNKNOWN'\n",
    "                used_marketcap = True\n",
    "            else:\n",
    "                # fallback: we do not have sharesOutstanding; use Close as proxy (not market cap)\n",
    "                hist['MarketCap_RAW'] = hist['Close']\n",
    "                hist['MarketCap_raw_unit'] = currency or 'UNKNOWN'\n",
    "                used_marketcap = False\n",
    "                print(f\"  NOTE: sharesOutstanding missing for {ytick}. Using Close price as proxy (unit={currency}).\")\n",
    "            # attempt to convert to USD if currency is not USD\n",
    "            series_dates = hist.index\n",
    "            usd_series = None\n",
    "            if currency and currency.upper() != 'USD':\n",
    "                fx = attempt_fx_conversion_series(series_dates, currency)\n",
    "                if fx is None:\n",
    "                    print(f\"  WARNING: could not fetch FX pair to convert {currency} -> USD for {name}. Values remain in {currency}.\")\n",
    "                    hist['MarketCap_USD'] = np.nan\n",
    "                    converted = False\n",
    "                else:\n",
    "                    # special handling for JPY: fx (JPY=X) typically gives USD/JPY (1 USD = X JPY)\n",
    "                    # To convert JPY_amount -> USD: USD = JPY_amount / (USD/JPY rate)\n",
    "                    if currency.upper() == 'JPY':\n",
    "                        hist['MarketCap_USD'] = hist['MarketCap_RAW'] / fx.reindex(hist.index).ffill()\n",
    "                    else:\n",
    "                        # assume fx series is direct rate currency->USD (e.g., EURUSD = 1.1 meaning 1 EUR = 1.1 USD).\n",
    "                        # If fx is USD/CUR (unlikely), results may be wrong — user should verify.\n",
    "                        hist['MarketCap_USD'] = hist['MarketCap_RAW'] * fx.reindex(hist.index).ffill()\n",
    "                    converted = True\n",
    "            else:\n",
    "                # already USD\n",
    "                hist['MarketCap_USD'] = hist['MarketCap_RAW']\n",
    "                converted = True\n",
    "\n",
    "            # get yearly last value (use last trading day of each year)\n",
    "            hist['Year'] = hist.index.year\n",
    "            yearly = hist.groupby('Year').agg({\n",
    "                'MarketCap_RAW': 'last',\n",
    "                'MarketCap_USD': 'last'\n",
    "            }).rename(columns={'MarketCap_RAW': 'RawValue', 'MarketCap_USD': 'USDValue'})\n",
    "\n",
    "            # store\n",
    "            per_company_yearly[name] = yearly\n",
    "            company_currency[name] = currency or 'UNKNOWN'\n",
    "            all_years.update(yearly.index.tolist())\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing {name} ({ytick}): {e}\")\n",
    "            failed_companies.append((name, ytick, str(e)))\n",
    "            continue\n",
    "\n",
    "    if not per_company_yearly:\n",
    "        print(\"No company data collected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Build master DataFrame with columns: Year, TotalValue_USD, Company1_USD, Company2_USD, ...\n",
    "    years = sorted(list(all_years))\n",
    "    master = pd.DataFrame(index=years)\n",
    "    master.index.name = 'Year'\n",
    "    # per-company columns\n",
    "    for name, yearly in per_company_yearly.items():\n",
    "        # Ensure index includes all years; reindex and keep NaN where missing\n",
    "        col_usd = yearly['USDValue'].reindex(years)\n",
    "        master[f\"{name}\"] = col_usd\n",
    "\n",
    "    # compute TotalValue_USD as sum of companies with USD available (skip NaNs)\n",
    "    master['TotalValue_USD'] = master.sum(axis=1, numeric_only=True)\n",
    "\n",
    "    # Save CSV\n",
    "    csv_name = \"yearly_values.csv\"\n",
    "    master.reset_index().to_csv(csv_name, index=False, float_format=\"%.2f\")\n",
    "    print(f\"\\nSaved CSV: {csv_name}\")\n",
    "\n",
    "    # Save XML\n",
    "    root = ET.Element(\"YearlyValues\")\n",
    "    for yr in master.index:\n",
    "        yel = ET.SubElement(root, \"Year\", attrib={\"value\": str(int(yr))})\n",
    "        total_el = ET.SubElement(yel, \"TotalValue_USD\")\n",
    "        total_val = master.at[yr, 'TotalValue_USD']\n",
    "        total_el.text = \"\" if pd.isna(total_val) else f\"{float(total_val):.2f}\"\n",
    "        # add each company\n",
    "        for name in per_company_yearly.keys():\n",
    "            comp_el = ET.SubElement(yel, \"Company\", attrib={\"name\": name})\n",
    "            val = master.at[yr, name]\n",
    "            if pd.isna(val):\n",
    "                comp_el.text = \"\"\n",
    "                comp_el.set(\"unit\", company_currency.get(name, \"UNKNOWN\"))\n",
    "                comp_el.set(\"converted_to_usd\", \"false\")\n",
    "            else:\n",
    "                comp_el.text = f\"{float(val):.2f}\"\n",
    "                comp_el.set(\"unit\", \"USD\")\n",
    "                comp_el.set(\"converted_to_usd\", \"true\")\n",
    "    tree = ET.ElementTree(root)\n",
    "    xml_name = \"yearly_values.xml\"\n",
    "    tree.write(xml_name, encoding=\"utf-8\", xml_declaration=True)\n",
    "    print(f\"Saved XML: {xml_name}\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\nDone. Summary:\")\n",
    "    print(f\"  Companies processed: {len(per_company_yearly)}\")\n",
    "    if failed_companies:\n",
    "        print(f\"  Failed companies: {len(failed_companies)} (see printed warnings above)\")\n",
    "    print(\"  Output files: yearly_values.csv, yearly_values.xml\")\n",
    "    print(\"\\nNotes:\")\n",
    "    print(\" - Values converted to USD when possible using Yahoo FX tickers (e.g., 'JPY=X').\")\n",
    "    print(\" - If sharesOutstanding was missing for a company, the script used Close price as a proxy (not a true market cap).\")\n",
    "    print(\" - Mixed-currency companies may not be included in TotalValue_USD if FX conversion failed.\")\n",
    "    print(\"\\nPlease check the CSV for per-company currency/unit issues before using totals for analysis.\")\n",
    "\n",
    "# Run\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e414514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#My Anime List scarping script to get all animes by year including their mean and their number of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a4d2de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading entire MyAnimeList anime database...\n",
      "\n",
      "Fetching page 1 ...\n",
      "Fetching page 2 ...\n",
      "Fetching page 3 ...\n",
      "Fetching page 4 ...\n",
      "Fetching page 5 ...\n",
      "Fetching page 6 ...\n",
      "Fetching page 7 ...\n",
      "Fetching page 8 ...\n",
      "Fetching page 9 ...\n",
      "Fetching page 10 ...\n",
      "Fetching page 11 ...\n",
      "Fetching page 12 ...\n",
      "Fetching page 13 ...\n",
      "Fetching page 14 ...\n",
      "Fetching page 15 ...\n",
      "Fetching page 16 ...\n",
      "Fetching page 17 ...\n",
      "Fetching page 18 ...\n",
      "Fetching page 19 ...\n",
      "Fetching page 20 ...\n",
      "Fetching page 21 ...\n",
      "Fetching page 22 ...\n",
      "Fetching page 23 ...\n",
      "Fetching page 24 ...\n",
      "Fetching page 25 ...\n",
      "Fetching page 26 ...\n",
      "Fetching page 27 ...\n",
      "Fetching page 28 ...\n",
      "Fetching page 29 ...\n",
      "Fetching page 30 ...\n",
      "Fetching page 31 ...\n",
      "Fetching page 32 ...\n",
      "Fetching page 33 ...\n",
      "Fetching page 34 ...\n",
      "Fetching page 35 ...\n",
      "Fetching page 36 ...\n",
      "Fetching page 37 ...\n",
      "Fetching page 38 ...\n",
      "Fetching page 39 ...\n",
      "Fetching page 40 ...\n",
      "Fetching page 41 ...\n",
      "Fetching page 42 ...\n",
      "Fetching page 43 ...\n",
      "Fetching page 44 ...\n",
      "Fetching page 45 ...\n",
      "Fetching page 46 ...\n",
      "Fetching page 47 ...\n",
      "Fetching page 48 ...\n",
      "Fetching page 49 ...\n",
      "Fetching page 50 ...\n",
      "Fetching page 51 ...\n",
      "Fetching page 52 ...\n",
      "Fetching page 53 ...\n",
      "Fetching page 54 ...\n",
      "Fetching page 55 ...\n",
      "Fetching page 56 ...\n",
      "Fetching page 57 ...\n",
      "Fetching page 58 ...\n",
      "Fetching page 59 ...\n",
      "Fetching page 60 ...\n",
      "Fetching page 61 ...\n",
      "Fetching page 62 ...\n",
      "Fetching page 63 ...\n",
      "Fetching page 64 ...\n",
      "Fetching page 65 ...\n",
      "Fetching page 66 ...\n",
      "Fetching page 67 ...\n",
      "Fetching page 68 ...\n",
      "Fetching page 69 ...\n",
      "Fetching page 70 ...\n",
      "Fetching page 71 ...\n",
      "Fetching page 72 ...\n",
      "Fetching page 73 ...\n",
      "Fetching page 74 ...\n",
      "Fetching page 75 ...\n",
      "Fetching page 76 ...\n",
      "Fetching page 77 ...\n",
      "Fetching page 78 ...\n",
      "Fetching page 79 ...\n",
      "Fetching page 80 ...\n",
      "Fetching page 81 ...\n",
      "Fetching page 82 ...\n",
      "Fetching page 83 ...\n",
      "Fetching page 84 ...\n",
      "Fetching page 85 ...\n",
      "Fetching page 86 ...\n",
      "Fetching page 87 ...\n",
      "Fetching page 88 ...\n",
      "Fetching page 89 ...\n",
      "Fetching page 90 ...\n",
      "Fetching page 91 ...\n",
      "Fetching page 92 ...\n",
      "Fetching page 93 ...\n",
      "Fetching page 94 ...\n",
      "Fetching page 95 ...\n",
      "Fetching page 96 ...\n",
      "Fetching page 97 ...\n",
      "Fetching page 98 ...\n",
      "Fetching page 99 ...\n",
      "Fetching page 100 ...\n",
      "Fetching page 101 ...\n",
      "Fetching page 102 ...\n",
      "Fetching page 103 ...\n",
      "Fetching page 104 ...\n",
      "Fetching page 105 ...\n",
      "Fetching page 106 ...\n",
      "Fetching page 107 ...\n",
      "Fetching page 108 ...\n",
      "Fetching page 109 ...\n",
      "Fetching page 110 ...\n",
      "Fetching page 111 ...\n",
      "Fetching page 112 ...\n",
      "Fetching page 113 ...\n",
      "Fetching page 114 ...\n",
      "Fetching page 115 ...\n",
      "Fetching page 116 ...\n",
      "Fetching page 117 ...\n",
      "Fetching page 118 ...\n",
      "Fetching page 119 ...\n",
      "Fetching page 120 ...\n",
      "Fetching page 121 ...\n",
      "Fetching page 122 ...\n",
      "Fetching page 123 ...\n",
      "Fetching page 124 ...\n",
      "Fetching page 125 ...\n",
      "Fetching page 126 ...\n",
      "Fetching page 127 ...\n",
      "Fetching page 128 ...\n",
      "Fetching page 129 ...\n",
      "Fetching page 130 ...\n",
      "Fetching page 131 ...\n",
      "Fetching page 132 ...\n",
      "Fetching page 133 ...\n",
      "Fetching page 134 ...\n",
      "Fetching page 135 ...\n",
      "Fetching page 136 ...\n",
      "Fetching page 137 ...\n",
      "Fetching page 138 ...\n",
      "Fetching page 139 ...\n",
      "Fetching page 140 ...\n",
      "Fetching page 141 ...\n",
      "Fetching page 142 ...\n",
      "Fetching page 143 ...\n",
      "Fetching page 144 ...\n",
      "Fetching page 145 ...\n",
      "Fetching page 146 ...\n",
      "Fetching page 147 ...\n",
      "Fetching page 148 ...\n",
      "Fetching page 149 ...\n",
      "Fetching page 150 ...\n",
      "Fetching page 151 ...\n",
      "Fetching page 152 ...\n",
      "Fetching page 153 ...\n",
      "Fetching page 154 ...\n",
      "Fetching page 155 ...\n",
      "Fetching page 156 ...\n",
      "Fetching page 157 ...\n",
      "Fetching page 158 ...\n",
      "Fetching page 159 ...\n",
      "Fetching page 160 ...\n",
      "Fetching page 161 ...\n",
      "Fetching page 162 ...\n",
      "Fetching page 163 ...\n",
      "Fetching page 164 ...\n",
      "Fetching page 165 ...\n",
      "Fetching page 166 ...\n",
      "Fetching page 167 ...\n",
      "Fetching page 168 ...\n",
      "Fetching page 169 ...\n",
      "Fetching page 170 ...\n",
      "Fetching page 171 ...\n",
      "Fetching page 172 ...\n",
      "Fetching page 173 ...\n",
      "Fetching page 174 ...\n",
      "Fetching page 175 ...\n",
      "Fetching page 176 ...\n",
      "Fetching page 177 ...\n",
      "Fetching page 178 ...\n",
      "Fetching page 179 ...\n",
      "Fetching page 180 ...\n",
      "Fetching page 181 ...\n",
      "Fetching page 182 ...\n",
      "Fetching page 183 ...\n",
      "Fetching page 184 ...\n",
      "Fetching page 185 ...\n",
      "Fetching page 186 ...\n",
      "Fetching page 187 ...\n",
      "Fetching page 188 ...\n",
      "Fetching page 189 ...\n",
      "Fetching page 190 ...\n",
      "Fetching page 191 ...\n",
      "Fetching page 192 ...\n",
      "Fetching page 193 ...\n",
      "Fetching page 194 ...\n",
      "Fetching page 195 ...\n",
      "Fetching page 196 ...\n",
      "Fetching page 197 ...\n",
      "Fetching page 198 ...\n",
      "Fetching page 199 ...\n",
      "Fetching page 200 ...\n",
      "Fetching page 201 ...\n",
      "Fetching page 202 ...\n",
      "Fetching page 203 ...\n",
      "Fetching page 204 ...\n",
      "Fetching page 205 ...\n",
      "Fetching page 206 ...\n",
      "Fetching page 207 ...\n",
      "Fetching page 208 ...\n",
      "Fetching page 209 ...\n",
      "Fetching page 210 ...\n",
      "Fetching page 211 ...\n",
      "Fetching page 212 ...\n",
      "Fetching page 213 ...\n",
      "Fetching page 214 ...\n",
      "Fetching page 215 ...\n",
      "Fetching page 216 ...\n",
      "Fetching page 217 ...\n",
      "Fetching page 218 ...\n",
      "Fetching page 219 ...\n",
      "Fetching page 220 ...\n",
      "Fetching page 221 ...\n",
      "Fetching page 222 ...\n",
      "Fetching page 223 ...\n",
      "Fetching page 224 ...\n",
      "Fetching page 225 ...\n",
      "Fetching page 226 ...\n",
      "Fetching page 227 ...\n",
      "Fetching page 228 ...\n",
      "Fetching page 229 ...\n",
      "Fetching page 230 ...\n",
      "Fetching page 231 ...\n",
      "Fetching page 232 ...\n",
      "Fetching page 233 ...\n",
      "Fetching page 234 ...\n",
      "Fetching page 235 ...\n",
      "Fetching page 236 ...\n",
      "Fetching page 237 ...\n",
      "Fetching page 238 ...\n",
      "Fetching page 239 ...\n",
      "Fetching page 240 ...\n",
      "Fetching page 241 ...\n",
      "Fetching page 242 ...\n",
      "Fetching page 243 ...\n",
      "Fetching page 244 ...\n",
      "Fetching page 245 ...\n",
      "Fetching page 246 ...\n",
      "Fetching page 247 ...\n",
      "Fetching page 248 ...\n",
      "Fetching page 249 ...\n",
      "Fetching page 250 ...\n",
      "Fetching page 251 ...\n",
      "Fetching page 252 ...\n",
      "Fetching page 253 ...\n",
      "Fetching page 254 ...\n",
      "Fetching page 255 ...\n",
      "Fetching page 256 ...\n",
      "Fetching page 257 ...\n",
      "Fetching page 258 ...\n",
      "Fetching page 259 ...\n",
      "Fetching page 260 ...\n",
      "Fetching page 261 ...\n",
      "Fetching page 262 ...\n",
      "Fetching page 263 ...\n",
      "Fetching page 264 ...\n",
      "Fetching page 265 ...\n",
      "Fetching page 266 ...\n",
      "Fetching page 267 ...\n",
      "Fetching page 268 ...\n",
      "Fetching page 269 ...\n",
      "Fetching page 270 ...\n",
      "Fetching page 271 ...\n",
      "Fetching page 272 ...\n",
      "Fetching page 273 ...\n",
      "Fetching page 274 ...\n",
      "Fetching page 275 ...\n",
      "Fetching page 276 ...\n",
      "Fetching page 277 ...\n",
      "Fetching page 278 ...\n",
      "Fetching page 279 ...\n",
      "Fetching page 280 ...\n",
      "Fetching page 281 ...\n",
      "Fetching page 282 ...\n",
      "Fetching page 283 ...\n",
      "Fetching page 284 ...\n",
      "Fetching page 285 ...\n",
      "Fetching page 286 ...\n",
      "Fetching page 287 ...\n",
      "Fetching page 288 ...\n",
      "Fetching page 289 ...\n",
      "Fetching page 290 ...\n",
      "Fetching page 291 ...\n",
      "Fetching page 292 ...\n",
      "Fetching page 293 ...\n",
      "Fetching page 294 ...\n",
      "Fetching page 295 ...\n",
      "Fetching page 296 ...\n",
      "Fetching page 297 ...\n",
      "Fetching page 298 ...\n",
      "Fetching page 299 ...\n",
      "Fetching page 300 ...\n",
      "Fetching page 301 ...\n",
      "Fetching page 302 ...\n",
      "Fetching page 303 ...\n",
      "Fetching page 304 ...\n",
      "Fetching page 305 ...\n",
      "Fetching page 306 ...\n",
      "Fetching page 307 ...\n",
      "Fetching page 308 ...\n",
      "Fetching page 309 ...\n",
      "Fetching page 310 ...\n",
      "Fetching page 311 ...\n",
      "Fetching page 312 ...\n",
      "Fetching page 313 ...\n",
      "Fetching page 314 ...\n",
      "Fetching page 315 ...\n",
      "Fetching page 316 ...\n",
      "Fetching page 317 ...\n",
      "Fetching page 318 ...\n",
      "Fetching page 319 ...\n",
      "Fetching page 320 ...\n",
      "Fetching page 321 ...\n",
      "Fetching page 322 ...\n",
      "Fetching page 323 ...\n",
      "Fetching page 324 ...\n",
      "Fetching page 325 ...\n",
      "Fetching page 326 ...\n",
      "Fetching page 327 ...\n",
      "Fetching page 328 ...\n",
      "Fetching page 329 ...\n",
      "Fetching page 330 ...\n",
      "Fetching page 331 ...\n",
      "Fetching page 332 ...\n",
      "Fetching page 333 ...\n",
      "Fetching page 334 ...\n",
      "Fetching page 335 ...\n",
      "Fetching page 336 ...\n",
      "Fetching page 337 ...\n",
      "Fetching page 338 ...\n",
      "Fetching page 339 ...\n",
      "Fetching page 340 ...\n",
      "Fetching page 341 ...\n",
      "Fetching page 342 ...\n",
      "Fetching page 343 ...\n",
      "Fetching page 344 ...\n",
      "Fetching page 345 ...\n",
      "Fetching page 346 ...\n",
      "Fetching page 347 ...\n",
      "Fetching page 348 ...\n",
      "Fetching page 349 ...\n",
      "Fetching page 350 ...\n",
      "Fetching page 351 ...\n",
      "Fetching page 352 ...\n",
      "Fetching page 353 ...\n",
      "Fetching page 354 ...\n",
      "Fetching page 355 ...\n",
      "Fetching page 356 ...\n",
      "Fetching page 357 ...\n",
      "Fetching page 358 ...\n",
      "Fetching page 359 ...\n",
      "Fetching page 360 ...\n",
      "Fetching page 361 ...\n",
      "Fetching page 362 ...\n",
      "Fetching page 363 ...\n",
      "Fetching page 364 ...\n",
      "Fetching page 365 ...\n",
      "Fetching page 366 ...\n",
      "Fetching page 367 ...\n",
      "Fetching page 368 ...\n",
      "Fetching page 369 ...\n",
      "Fetching page 370 ...\n",
      "Fetching page 371 ...\n",
      "Fetching page 372 ...\n",
      "Fetching page 373 ...\n",
      "Fetching page 374 ...\n",
      "Fetching page 375 ...\n",
      "Fetching page 376 ...\n",
      "Fetching page 377 ...\n",
      "Fetching page 378 ...\n",
      "Fetching page 379 ...\n",
      "Fetching page 380 ...\n",
      "Fetching page 381 ...\n",
      "Fetching page 382 ...\n",
      "Fetching page 383 ...\n",
      "Fetching page 384 ...\n",
      "Fetching page 385 ...\n",
      "Fetching page 386 ...\n",
      "Fetching page 387 ...\n",
      "Fetching page 388 ...\n",
      "Fetching page 389 ...\n",
      "Fetching page 390 ...\n",
      "Fetching page 391 ...\n",
      "Fetching page 392 ...\n",
      "Fetching page 393 ...\n",
      "Fetching page 394 ...\n",
      "Fetching page 395 ...\n",
      "Fetching page 396 ...\n",
      "Fetching page 397 ...\n",
      "Fetching page 398 ...\n",
      "Fetching page 399 ...\n",
      "Fetching page 400 ...\n",
      "Fetching page 401 ...\n",
      "Fetching page 402 ...\n",
      "Fetching page 403 ...\n",
      "Fetching page 404 ...\n",
      "Fetching page 405 ...\n",
      "Fetching page 406 ...\n",
      "Fetching page 407 ...\n",
      "Fetching page 408 ...\n",
      "Fetching page 409 ...\n",
      "Fetching page 410 ...\n",
      "Fetching page 411 ...\n",
      "Fetching page 412 ...\n",
      "Fetching page 413 ...\n",
      "Fetching page 414 ...\n",
      "Fetching page 415 ...\n",
      "Fetching page 416 ...\n",
      "Fetching page 417 ...\n",
      "Fetching page 418 ...\n",
      "Fetching page 419 ...\n",
      "Fetching page 420 ...\n",
      "Fetching page 421 ...\n",
      "Fetching page 422 ...\n",
      "Fetching page 423 ...\n",
      "Fetching page 424 ...\n",
      "Fetching page 425 ...\n",
      "Fetching page 426 ...\n",
      "Fetching page 427 ...\n",
      "Fetching page 428 ...\n",
      "Fetching page 429 ...\n",
      "Fetching page 430 ...\n",
      "Fetching page 431 ...\n",
      "Fetching page 432 ...\n",
      "Fetching page 433 ...\n",
      "Fetching page 434 ...\n",
      "Fetching page 435 ...\n",
      "Fetching page 436 ...\n",
      "Fetching page 437 ...\n",
      "Fetching page 438 ...\n",
      "Fetching page 439 ...\n",
      "Fetching page 440 ...\n",
      "Fetching page 441 ...\n",
      "Fetching page 442 ...\n",
      "Fetching page 443 ...\n",
      "Fetching page 444 ...\n",
      "Fetching page 445 ...\n",
      "Fetching page 446 ...\n",
      "Fetching page 447 ...\n",
      "Fetching page 448 ...\n",
      "Fetching page 449 ...\n",
      "Fetching page 450 ...\n",
      "Fetching page 451 ...\n",
      "Fetching page 452 ...\n",
      "Fetching page 453 ...\n",
      "Fetching page 454 ...\n",
      "Fetching page 455 ...\n",
      "Fetching page 456 ...\n",
      "Fetching page 457 ...\n",
      "Fetching page 458 ...\n",
      "Fetching page 459 ...\n",
      "Fetching page 460 ...\n",
      "Fetching page 461 ...\n",
      "Fetching page 462 ...\n",
      "Fetching page 463 ...\n",
      "Fetching page 464 ...\n",
      "Fetching page 465 ...\n",
      "Fetching page 466 ...\n",
      "Fetching page 467 ...\n",
      "Fetching page 468 ...\n",
      "Fetching page 469 ...\n",
      "Fetching page 470 ...\n",
      "Fetching page 471 ...\n",
      "Fetching page 472 ...\n",
      "Fetching page 473 ...\n",
      "Fetching page 474 ...\n",
      "Fetching page 475 ...\n",
      "Fetching page 476 ...\n",
      "Fetching page 477 ...\n",
      "Fetching page 478 ...\n",
      "Fetching page 479 ...\n",
      "Fetching page 480 ...\n",
      "Fetching page 481 ...\n",
      "Fetching page 482 ...\n",
      "Fetching page 483 ...\n",
      "Fetching page 484 ...\n",
      "Fetching page 485 ...\n",
      "Fetching page 486 ...\n",
      "Fetching page 487 ...\n",
      "Fetching page 488 ...\n",
      "Fetching page 489 ...\n",
      "Fetching page 490 ...\n",
      "Fetching page 491 ...\n",
      "Fetching page 492 ...\n",
      "Fetching page 493 ...\n",
      "Fetching page 494 ...\n",
      "Fetching page 495 ...\n",
      "Fetching page 496 ...\n",
      "Fetching page 497 ...\n",
      "Fetching page 498 ...\n",
      "Fetching page 499 ...\n",
      "Fetching page 500 ...\n",
      "Fetching page 501 ...\n",
      "Fetching page 502 ...\n",
      "Fetching page 503 ...\n",
      "Fetching page 504 ...\n",
      "Fetching page 505 ...\n",
      "Fetching page 506 ...\n",
      "Fetching page 507 ...\n",
      "Fetching page 508 ...\n",
      "Fetching page 509 ...\n",
      "Fetching page 510 ...\n",
      "Fetching page 511 ...\n",
      "Fetching page 512 ...\n",
      "Fetching page 513 ...\n",
      "Fetching page 514 ...\n",
      "Fetching page 515 ...\n",
      "Fetching page 516 ...\n",
      "Fetching page 517 ...\n",
      "Fetching page 518 ...\n",
      "Fetching page 519 ...\n",
      "Fetching page 520 ...\n",
      "Fetching page 521 ...\n",
      "Fetching page 522 ...\n",
      "Fetching page 523 ...\n",
      "Fetching page 524 ...\n",
      "Fetching page 525 ...\n",
      "Fetching page 526 ...\n",
      "Fetching page 527 ...\n",
      "Fetching page 528 ...\n",
      "Fetching page 529 ...\n",
      "Fetching page 530 ...\n",
      "Fetching page 531 ...\n",
      "Fetching page 532 ...\n",
      "Fetching page 533 ...\n",
      "Fetching page 534 ...\n",
      "Fetching page 535 ...\n",
      "Fetching page 536 ...\n",
      "Fetching page 537 ...\n",
      "Fetching page 538 ...\n",
      "Fetching page 539 ...\n",
      "Fetching page 540 ...\n",
      "Fetching page 541 ...\n",
      "Fetching page 542 ...\n",
      "Fetching page 543 ...\n",
      "Fetching page 544 ...\n",
      "Fetching page 545 ...\n",
      "Fetching page 546 ...\n",
      "Fetching page 547 ...\n",
      "Fetching page 548 ...\n",
      "Fetching page 549 ...\n",
      "Fetching page 550 ...\n",
      "Fetching page 551 ...\n",
      "Fetching page 552 ...\n",
      "Fetching page 553 ...\n",
      "Fetching page 554 ...\n",
      "Fetching page 555 ...\n",
      "Fetching page 556 ...\n",
      "Fetching page 557 ...\n",
      "Fetching page 558 ...\n",
      "Fetching page 559 ...\n",
      "Fetching page 560 ...\n",
      "Fetching page 561 ...\n",
      "Fetching page 562 ...\n",
      "Fetching page 563 ...\n",
      "Fetching page 564 ...\n",
      "Fetching page 565 ...\n",
      "Fetching page 566 ...\n",
      "Fetching page 567 ...\n",
      "Fetching page 568 ...\n",
      "Fetching page 569 ...\n",
      "Fetching page 570 ...\n",
      "Fetching page 571 ...\n",
      "Fetching page 572 ...\n",
      "Fetching page 573 ...\n",
      "Fetching page 574 ...\n",
      "Fetching page 575 ...\n",
      "Fetching page 576 ...\n",
      "Fetching page 577 ...\n",
      "Fetching page 578 ...\n",
      "Fetching page 579 ...\n",
      "Fetching page 580 ...\n",
      "Fetching page 581 ...\n",
      "Fetching page 582 ...\n",
      "Fetching page 583 ...\n",
      "Fetching page 584 ...\n",
      "Fetching page 585 ...\n",
      "Fetching page 586 ...\n",
      "Fetching page 587 ...\n",
      "Fetching page 588 ...\n",
      "Fetching page 589 ...\n",
      "Fetching page 590 ...\n",
      "Fetching page 591 ...\n",
      "Fetching page 592 ...\n",
      "Fetching page 593 ...\n",
      "Fetching page 594 ...\n",
      "Fetching page 595 ...\n",
      "Fetching page 596 ...\n",
      "Fetching page 597 ...\n",
      "Fetching page 598 ...\n",
      "Fetching page 599 ...\n",
      "Fetching page 600 ...\n",
      "Fetching page 601 ...\n",
      "Fetching page 602 ...\n",
      "Fetching page 603 ...\n",
      "Fetching page 604 ...\n",
      "Fetching page 605 ...\n",
      "Fetching page 606 ...\n",
      "Fetching page 607 ...\n",
      "Fetching page 608 ...\n",
      "Fetching page 609 ...\n",
      "Fetching page 610 ...\n",
      "Fetching page 611 ...\n",
      "Fetching page 612 ...\n",
      "Fetching page 613 ...\n",
      "Fetching page 614 ...\n",
      "Fetching page 615 ...\n",
      "Fetching page 616 ...\n",
      "Fetching page 617 ...\n",
      "Fetching page 618 ...\n",
      "Fetching page 619 ...\n",
      "Fetching page 620 ...\n",
      "Fetching page 621 ...\n",
      "Fetching page 622 ...\n",
      "Fetching page 623 ...\n",
      "Fetching page 624 ...\n",
      "Fetching page 625 ...\n",
      "Fetching page 626 ...\n",
      "Fetching page 627 ...\n",
      "Fetching page 628 ...\n",
      "Fetching page 629 ...\n",
      "Fetching page 630 ...\n",
      "Fetching page 631 ...\n",
      "Fetching page 632 ...\n",
      "Fetching page 633 ...\n",
      "Fetching page 634 ...\n",
      "Fetching page 635 ...\n",
      "Fetching page 636 ...\n",
      "Fetching page 637 ...\n",
      "Fetching page 638 ...\n",
      "Fetching page 639 ...\n",
      "Fetching page 640 ...\n",
      "Fetching page 641 ...\n",
      "Fetching page 642 ...\n",
      "Fetching page 643 ...\n",
      "Fetching page 644 ...\n",
      "Fetching page 645 ...\n",
      "Fetching page 646 ...\n",
      "Fetching page 647 ...\n",
      "Fetching page 648 ...\n",
      "Fetching page 649 ...\n",
      "Fetching page 650 ...\n",
      "Fetching page 651 ...\n",
      "Fetching page 652 ...\n",
      "Fetching page 653 ...\n",
      "Fetching page 654 ...\n",
      "Fetching page 655 ...\n",
      "Fetching page 656 ...\n",
      "Fetching page 657 ...\n",
      "Fetching page 658 ...\n",
      "Fetching page 659 ...\n",
      "Fetching page 660 ...\n",
      "Fetching page 661 ...\n",
      "Fetching page 662 ...\n",
      "Fetching page 663 ...\n",
      "Fetching page 664 ...\n",
      "Fetching page 665 ...\n",
      "Fetching page 666 ...\n",
      "Fetching page 667 ...\n",
      "Fetching page 668 ...\n",
      "Fetching page 669 ...\n",
      "Fetching page 670 ...\n",
      "Fetching page 671 ...\n",
      "Fetching page 672 ...\n",
      "Fetching page 673 ...\n",
      "Fetching page 674 ...\n",
      "Fetching page 675 ...\n",
      "Fetching page 676 ...\n",
      "Fetching page 677 ...\n",
      "Fetching page 678 ...\n",
      "Fetching page 679 ...\n",
      "Fetching page 680 ...\n",
      "Fetching page 681 ...\n",
      "Fetching page 682 ...\n",
      "Fetching page 683 ...\n",
      "Fetching page 684 ...\n",
      "Fetching page 685 ...\n",
      "Fetching page 686 ...\n",
      "Fetching page 687 ...\n",
      "Fetching page 688 ...\n",
      "Fetching page 689 ...\n",
      "Fetching page 690 ...\n",
      "Fetching page 691 ...\n",
      "Fetching page 692 ...\n",
      "Fetching page 693 ...\n",
      "Fetching page 694 ...\n",
      "Fetching page 695 ...\n",
      "Fetching page 696 ...\n",
      "Fetching page 697 ...\n",
      "Fetching page 698 ...\n",
      "Fetching page 699 ...\n",
      "Fetching page 700 ...\n",
      "Fetching page 701 ...\n",
      "Fetching page 702 ...\n",
      "Fetching page 703 ...\n",
      "Fetching page 704 ...\n",
      "Fetching page 705 ...\n",
      "Fetching page 706 ...\n",
      "Fetching page 707 ...\n",
      "Fetching page 708 ...\n",
      "Fetching page 709 ...\n",
      "Fetching page 710 ...\n",
      "Fetching page 711 ...\n",
      "Fetching page 712 ...\n",
      "Fetching page 713 ...\n",
      "Fetching page 714 ...\n",
      "Fetching page 715 ...\n",
      "Fetching page 716 ...\n",
      "Fetching page 717 ...\n",
      "Fetching page 718 ...\n",
      "Fetching page 719 ...\n",
      "Fetching page 720 ...\n",
      "Fetching page 721 ...\n",
      "Fetching page 722 ...\n",
      "Fetching page 723 ...\n",
      "Fetching page 724 ...\n",
      "Fetching page 725 ...\n",
      "Fetching page 726 ...\n",
      "Fetching page 727 ...\n",
      "Fetching page 728 ...\n",
      "Fetching page 729 ...\n",
      "Fetching page 730 ...\n",
      "Fetching page 731 ...\n",
      "Fetching page 732 ...\n",
      "Fetching page 733 ...\n",
      "Fetching page 734 ...\n",
      "Fetching page 735 ...\n",
      "Fetching page 736 ...\n",
      "Fetching page 737 ...\n",
      "Fetching page 738 ...\n",
      "Fetching page 739 ...\n",
      "Fetching page 740 ...\n",
      "Fetching page 741 ...\n",
      "Fetching page 742 ...\n",
      "Fetching page 743 ...\n",
      "Fetching page 744 ...\n",
      "Fetching page 745 ...\n",
      "Fetching page 746 ...\n",
      "Fetching page 747 ...\n",
      "Fetching page 748 ...\n",
      "Fetching page 749 ...\n",
      "Fetching page 750 ...\n",
      "Fetching page 751 ...\n",
      "Fetching page 752 ...\n",
      "Fetching page 753 ...\n",
      "Fetching page 754 ...\n",
      "Fetching page 755 ...\n",
      "Fetching page 756 ...\n",
      "Fetching page 757 ...\n",
      "Fetching page 758 ...\n",
      "Fetching page 759 ...\n",
      "Fetching page 760 ...\n",
      "Fetching page 761 ...\n",
      "Fetching page 762 ...\n",
      "Fetching page 763 ...\n",
      "Fetching page 764 ...\n",
      "Fetching page 765 ...\n",
      "Fetching page 766 ...\n",
      "Fetching page 767 ...\n",
      "Fetching page 768 ...\n",
      "Fetching page 769 ...\n",
      "Fetching page 770 ...\n",
      "Fetching page 771 ...\n",
      "Fetching page 772 ...\n",
      "Fetching page 773 ...\n",
      "Fetching page 774 ...\n",
      "Fetching page 775 ...\n",
      "Fetching page 776 ...\n",
      "Fetching page 777 ...\n",
      "Fetching page 778 ...\n",
      "Fetching page 779 ...\n",
      "Fetching page 780 ...\n",
      "Fetching page 781 ...\n",
      "Fetching page 782 ...\n",
      "Fetching page 783 ...\n",
      "Fetching page 784 ...\n",
      "Fetching page 785 ...\n",
      "Fetching page 786 ...\n",
      "Fetching page 787 ...\n",
      "Fetching page 788 ...\n",
      "Fetching page 789 ...\n",
      "Fetching page 790 ...\n",
      "Fetching page 791 ...\n",
      "Fetching page 792 ...\n",
      "Fetching page 793 ...\n",
      "Fetching page 794 ...\n",
      "Fetching page 795 ...\n",
      "Fetching page 796 ...\n",
      "Fetching page 797 ...\n",
      "Fetching page 798 ...\n",
      "Fetching page 799 ...\n",
      "Fetching page 800 ...\n",
      "Fetching page 801 ...\n",
      "Fetching page 802 ...\n",
      "Fetching page 803 ...\n",
      "Fetching page 804 ...\n",
      "Fetching page 805 ...\n",
      "Fetching page 806 ...\n",
      "Fetching page 807 ...\n",
      "Fetching page 808 ...\n",
      "Fetching page 809 ...\n",
      "Fetching page 810 ...\n",
      "Fetching page 811 ...\n",
      "Fetching page 812 ...\n",
      "Fetching page 813 ...\n",
      "Fetching page 814 ...\n",
      "Fetching page 815 ...\n",
      "Fetching page 816 ...\n",
      "Fetching page 817 ...\n",
      "Fetching page 818 ...\n",
      "Fetching page 819 ...\n",
      "Fetching page 820 ...\n",
      "Fetching page 821 ...\n",
      "Fetching page 822 ...\n",
      "Fetching page 823 ...\n",
      "Fetching page 824 ...\n",
      "Fetching page 825 ...\n",
      "Fetching page 826 ...\n",
      "Fetching page 827 ...\n",
      "Fetching page 828 ...\n",
      "Fetching page 829 ...\n",
      "Fetching page 830 ...\n",
      "Fetching page 831 ...\n",
      "Fetching page 832 ...\n",
      "Fetching page 833 ...\n",
      "Fetching page 834 ...\n",
      "Fetching page 835 ...\n",
      "Fetching page 836 ...\n",
      "Fetching page 837 ...\n",
      "Fetching page 838 ...\n",
      "Fetching page 839 ...\n",
      "Fetching page 840 ...\n",
      "Fetching page 841 ...\n",
      "Fetching page 842 ...\n",
      "Fetching page 843 ...\n",
      "Fetching page 844 ...\n",
      "Fetching page 845 ...\n",
      "Fetching page 846 ...\n",
      "Fetching page 847 ...\n",
      "Fetching page 848 ...\n",
      "Fetching page 849 ...\n",
      "Fetching page 850 ...\n",
      "Fetching page 851 ...\n",
      "Fetching page 852 ...\n",
      "Fetching page 853 ...\n",
      "Fetching page 854 ...\n",
      "Fetching page 855 ...\n",
      "Fetching page 856 ...\n",
      "Fetching page 857 ...\n",
      "Fetching page 858 ...\n",
      "Fetching page 859 ...\n",
      "Fetching page 860 ...\n",
      "Fetching page 861 ...\n",
      "Fetching page 862 ...\n",
      "Fetching page 863 ...\n",
      "Fetching page 864 ...\n",
      "Fetching page 865 ...\n",
      "Fetching page 866 ...\n",
      "Fetching page 867 ...\n",
      "Fetching page 868 ...\n",
      "Fetching page 869 ...\n",
      "Fetching page 870 ...\n",
      "Fetching page 871 ...\n",
      "Fetching page 872 ...\n",
      "Fetching page 873 ...\n",
      "Fetching page 874 ...\n",
      "Fetching page 875 ...\n",
      "Fetching page 876 ...\n",
      "Fetching page 877 ...\n",
      "Fetching page 878 ...\n",
      "Fetching page 879 ...\n",
      "Fetching page 880 ...\n",
      "Fetching page 881 ...\n",
      "Fetching page 882 ...\n",
      "Fetching page 883 ...\n",
      "Fetching page 884 ...\n",
      "Fetching page 885 ...\n",
      "Fetching page 886 ...\n",
      "Fetching page 887 ...\n",
      "Fetching page 888 ...\n",
      "Fetching page 889 ...\n",
      "Fetching page 890 ...\n",
      "Fetching page 891 ...\n",
      "Fetching page 892 ...\n",
      "Fetching page 893 ...\n",
      "Fetching page 894 ...\n",
      "Fetching page 895 ...\n",
      "Fetching page 896 ...\n",
      "Fetching page 897 ...\n",
      "Fetching page 898 ...\n",
      "Fetching page 899 ...\n",
      "Fetching page 900 ...\n",
      "Fetching page 901 ...\n",
      "Fetching page 902 ...\n",
      "Fetching page 903 ...\n",
      "Fetching page 904 ...\n",
      "Fetching page 905 ...\n",
      "Fetching page 906 ...\n",
      "Fetching page 907 ...\n",
      "Fetching page 908 ...\n",
      "Fetching page 909 ...\n",
      "Fetching page 910 ...\n",
      "Fetching page 911 ...\n",
      "Fetching page 912 ...\n",
      "Fetching page 913 ...\n",
      "Fetching page 914 ...\n",
      "Fetching page 915 ...\n",
      "Fetching page 916 ...\n",
      "Fetching page 917 ...\n",
      "Fetching page 918 ...\n",
      "Fetching page 919 ...\n",
      "Fetching page 920 ...\n",
      "Fetching page 921 ...\n",
      "Fetching page 922 ...\n",
      "Fetching page 923 ...\n",
      "Fetching page 924 ...\n",
      "Fetching page 925 ...\n",
      "Fetching page 926 ...\n",
      "Fetching page 927 ...\n",
      "Fetching page 928 ...\n",
      "Fetching page 929 ...\n",
      "Fetching page 930 ...\n",
      "Fetching page 931 ...\n",
      "Fetching page 932 ...\n",
      "Fetching page 933 ...\n",
      "Fetching page 934 ...\n",
      "Fetching page 935 ...\n",
      "Fetching page 936 ...\n",
      "Fetching page 937 ...\n",
      "Fetching page 938 ...\n",
      "Fetching page 939 ...\n",
      "Fetching page 940 ...\n",
      "Fetching page 941 ...\n",
      "Fetching page 942 ...\n",
      "Fetching page 943 ...\n",
      "Fetching page 944 ...\n",
      "Fetching page 945 ...\n",
      "Fetching page 946 ...\n",
      "Fetching page 947 ...\n",
      "Fetching page 948 ...\n",
      "Fetching page 949 ...\n",
      "Fetching page 950 ...\n",
      "Fetching page 951 ...\n",
      "Fetching page 952 ...\n",
      "Fetching page 953 ...\n",
      "Fetching page 954 ...\n",
      "Fetching page 955 ...\n",
      "Fetching page 956 ...\n",
      "Fetching page 957 ...\n",
      "Fetching page 958 ...\n",
      "Fetching page 959 ...\n",
      "Fetching page 960 ...\n",
      "Fetching page 961 ...\n",
      "Fetching page 962 ...\n",
      "Fetching page 963 ...\n",
      "Fetching page 964 ...\n",
      "Fetching page 965 ...\n",
      "Fetching page 966 ...\n",
      "Fetching page 967 ...\n",
      "Fetching page 968 ...\n",
      "Fetching page 969 ...\n",
      "Fetching page 970 ...\n",
      "Fetching page 971 ...\n",
      "Fetching page 972 ...\n",
      "Fetching page 973 ...\n",
      "Fetching page 974 ...\n",
      "Fetching page 975 ...\n",
      "Fetching page 976 ...\n",
      "Fetching page 977 ...\n",
      "Fetching page 978 ...\n",
      "Fetching page 979 ...\n",
      "Fetching page 980 ...\n",
      "Fetching page 981 ...\n",
      "Fetching page 982 ...\n",
      "Fetching page 983 ...\n",
      "Fetching page 984 ...\n",
      "Fetching page 985 ...\n",
      "Fetching page 986 ...\n",
      "Fetching page 987 ...\n",
      "Fetching page 988 ...\n",
      "Fetching page 989 ...\n",
      "Fetching page 990 ...\n",
      "Fetching page 991 ...\n",
      "Fetching page 992 ...\n",
      "Fetching page 993 ...\n",
      "Fetching page 994 ...\n",
      "Fetching page 995 ...\n",
      "Fetching page 996 ...\n",
      "Fetching page 997 ...\n",
      "Fetching page 998 ...\n",
      "Fetching page 999 ...\n",
      "Fetching page 1000 ...\n",
      "Fetching page 1001 ...\n",
      "Fetching page 1002 ...\n",
      "Fetching page 1003 ...\n",
      "Fetching page 1004 ...\n",
      "Fetching page 1005 ...\n",
      "Fetching page 1006 ...\n",
      "Fetching page 1007 ...\n",
      "Fetching page 1008 ...\n",
      "Fetching page 1009 ...\n",
      "Fetching page 1010 ...\n",
      "Fetching page 1011 ...\n",
      "Fetching page 1012 ...\n",
      "Fetching page 1013 ...\n",
      "Fetching page 1014 ...\n",
      "Fetching page 1015 ...\n",
      "Fetching page 1016 ...\n",
      "Fetching page 1017 ...\n",
      "Fetching page 1018 ...\n",
      "Fetching page 1019 ...\n",
      "Fetching page 1020 ...\n",
      "Fetching page 1021 ...\n",
      "Fetching page 1022 ...\n",
      "Fetching page 1023 ...\n",
      "Fetching page 1024 ...\n",
      "Fetching page 1025 ...\n",
      "Fetching page 1026 ...\n",
      "Fetching page 1027 ...\n",
      "Fetching page 1028 ...\n",
      "Fetching page 1029 ...\n",
      "Fetching page 1030 ...\n",
      "Fetching page 1031 ...\n",
      "Fetching page 1032 ...\n",
      "Fetching page 1033 ...\n",
      "Fetching page 1034 ...\n",
      "Fetching page 1035 ...\n",
      "Fetching page 1036 ...\n",
      "Fetching page 1037 ...\n",
      "Fetching page 1038 ...\n",
      "Fetching page 1039 ...\n",
      "Fetching page 1040 ...\n",
      "Fetching page 1041 ...\n",
      "Fetching page 1042 ...\n",
      "Fetching page 1043 ...\n",
      "Fetching page 1044 ...\n",
      "Fetching page 1045 ...\n",
      "Fetching page 1046 ...\n",
      "Fetching page 1047 ...\n",
      "Fetching page 1048 ...\n",
      "Fetching page 1049 ...\n",
      "Fetching page 1050 ...\n",
      "Fetching page 1051 ...\n",
      "Fetching page 1052 ...\n",
      "Fetching page 1053 ...\n",
      "Fetching page 1054 ...\n",
      "Fetching page 1055 ...\n",
      "Fetching page 1056 ...\n",
      "Fetching page 1057 ...\n",
      "Fetching page 1058 ...\n",
      "Fetching page 1059 ...\n",
      "Fetching page 1060 ...\n",
      "Fetching page 1061 ...\n",
      "Fetching page 1062 ...\n",
      "Fetching page 1063 ...\n",
      "Fetching page 1064 ...\n",
      "Fetching page 1065 ...\n",
      "Fetching page 1066 ...\n",
      "Fetching page 1067 ...\n",
      "Fetching page 1068 ...\n",
      "Fetching page 1069 ...\n",
      "Fetching page 1070 ...\n",
      "Fetching page 1071 ...\n",
      "Fetching page 1072 ...\n",
      "Fetching page 1073 ...\n",
      "Fetching page 1074 ...\n",
      "Fetching page 1075 ...\n",
      "Fetching page 1076 ...\n",
      "Fetching page 1077 ...\n",
      "Fetching page 1078 ...\n",
      "Fetching page 1079 ...\n",
      "Fetching page 1080 ...\n",
      "Fetching page 1081 ...\n",
      "Fetching page 1082 ...\n",
      "Fetching page 1083 ...\n",
      "Fetching page 1084 ...\n",
      "Fetching page 1085 ...\n",
      "Fetching page 1086 ...\n",
      "Fetching page 1087 ...\n",
      "Fetching page 1088 ...\n",
      "Fetching page 1089 ...\n",
      "Fetching page 1090 ...\n",
      "Fetching page 1091 ...\n",
      "Fetching page 1092 ...\n",
      "Fetching page 1093 ...\n",
      "Fetching page 1094 ...\n",
      "Fetching page 1095 ...\n",
      "Fetching page 1096 ...\n",
      "Fetching page 1097 ...\n",
      "Fetching page 1098 ...\n",
      "Fetching page 1099 ...\n",
      "Fetching page 1100 ...\n",
      "Fetching page 1101 ...\n",
      "Fetching page 1102 ...\n",
      "Fetching page 1103 ...\n",
      "Fetching page 1104 ...\n",
      "Fetching page 1105 ...\n",
      "Fetching page 1106 ...\n",
      "Fetching page 1107 ...\n",
      "Fetching page 1108 ...\n",
      "Fetching page 1109 ...\n",
      "Fetching page 1110 ...\n",
      "Fetching page 1111 ...\n",
      "Fetching page 1112 ...\n",
      "Fetching page 1113 ...\n",
      "Fetching page 1114 ...\n",
      "Fetching page 1115 ...\n",
      "Fetching page 1116 ...\n",
      "Fetching page 1117 ...\n",
      "Fetching page 1118 ...\n",
      "Fetching page 1119 ...\n",
      "Fetching page 1120 ...\n",
      "Fetching page 1121 ...\n",
      "Fetching page 1122 ...\n",
      "Fetching page 1123 ...\n",
      "Fetching page 1124 ...\n",
      "Fetching page 1125 ...\n",
      "Fetching page 1126 ...\n",
      "Fetching page 1127 ...\n",
      "Fetching page 1128 ...\n",
      "Fetching page 1129 ...\n",
      "Fetching page 1130 ...\n",
      "Fetching page 1131 ...\n",
      "Fetching page 1132 ...\n",
      "Fetching page 1133 ...\n",
      "Fetching page 1134 ...\n",
      "Fetching page 1135 ...\n",
      "Fetching page 1136 ...\n",
      "Fetching page 1137 ...\n",
      "Fetching page 1138 ...\n",
      "Fetching page 1139 ...\n",
      "Fetching page 1140 ...\n",
      "Fetching page 1141 ...\n",
      "Fetching page 1142 ...\n",
      "Fetching page 1143 ...\n",
      "Fetching page 1144 ...\n",
      "Fetching page 1145 ...\n",
      "Fetching page 1146 ...\n",
      "Fetching page 1147 ...\n",
      "Fetching page 1148 ...\n",
      "Fetching page 1149 ...\n",
      "Fetching page 1150 ...\n",
      "Fetching page 1151 ...\n",
      "Fetching page 1152 ...\n",
      "Fetching page 1153 ...\n",
      "Fetching page 1154 ...\n",
      "Fetching page 1155 ...\n",
      "Fetching page 1156 ...\n",
      "Fetching page 1157 ...\n",
      "Fetching page 1158 ...\n",
      "Fetching page 1159 ...\n",
      "Fetching page 1160 ...\n",
      "Fetching page 1161 ...\n",
      "Fetching page 1162 ...\n",
      "Fetching page 1163 ...\n",
      "Fetching page 1164 ...\n",
      "Fetching page 1165 ...\n",
      "Fetching page 1166 ...\n",
      "Fetching page 1167 ...\n",
      "Fetching page 1168 ...\n",
      "Fetching page 1169 ...\n",
      "Fetching page 1170 ...\n",
      "Fetching page 1171 ...\n",
      "Fetching page 1172 ...\n",
      "Fetching page 1173 ...\n",
      "Fetching page 1174 ...\n",
      "Fetching page 1175 ...\n",
      "Fetching page 1176 ...\n",
      "Fetching page 1177 ...\n",
      "Fetching page 1178 ...\n",
      "\n",
      "✔ Finished downloading all anime!\n",
      "\n",
      "✔ Saved:\n",
      " - all_anime_raw.csv (full dataset)\n",
      " - anime_year_stats.csv (yearly stats)\n",
      "      year  anime_count  mean_score\n",
      "0   1961.0            1    6.080000\n",
      "1   1962.0            1    6.040000\n",
      "2   1963.0            5    6.148000\n",
      "3   1964.0            4    5.966667\n",
      "4   1965.0           12    5.945556\n",
      "..     ...          ...         ...\n",
      "62  2023.0          285    7.031909\n",
      "63  2024.0          284    7.054889\n",
      "64  2025.0          295    6.947414\n",
      "65  2026.0           94         NaN\n",
      "66  2027.0            2         NaN\n",
      "\n",
      "[67 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "all_anime = []\n",
    "page = 1\n",
    "\n",
    "print(\"Downloading entire MyAnimeList anime database...\\n\")\n",
    "\n",
    "while True:\n",
    "    url = f\"https://api.jikan.moe/v4/anime?page={page}\"\n",
    "    print(f\"Fetching page {page} ...\")\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Avoid rate limits\n",
    "    if response.status_code == 429:\n",
    "        print(\"Rate limited, waiting 2 seconds...\")\n",
    "        time.sleep(2)\n",
    "        continue\n",
    "\n",
    "    data = response.json()\n",
    "\n",
    "    if \"data\" not in data or len(data[\"data\"]) == 0:\n",
    "        print(\"\\n✔ Finished downloading all anime!\")\n",
    "        break\n",
    "\n",
    "    # Extract relevant info\n",
    "    for anime in data[\"data\"]:\n",
    "        title = anime.get(\"title\")\n",
    "        score = anime.get(\"score\")\n",
    "        year = anime.get(\"year\")   # None if unknown\n",
    "\n",
    "        all_anime.append({\n",
    "            \"title\": title,\n",
    "            \"score\": score,\n",
    "            \"year\": year\n",
    "        })\n",
    "\n",
    "    page += 1\n",
    "    time.sleep(1.1)  # Jikan rate limit\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_anime)\n",
    "\n",
    "# Clean data\n",
    "df = df.dropna(subset=[\"year\"])   # remove entries without year\n",
    "\n",
    "# Group by year\n",
    "result = df.groupby(\"year\").agg(\n",
    "    anime_count=(\"title\", \"count\"),\n",
    "    mean_score=(\"score\", \"mean\")\n",
    ").reset_index()\n",
    "\n",
    "# Save results\n",
    "df.to_csv(\"all_anime_raw.csv\", index=False)\n",
    "result.to_csv(\"anime_year_stats.csv\", index=False)\n",
    "\n",
    "print(\"\\n✔ Saved:\")\n",
    "print(\" - all_anime_raw.csv (full dataset)\")\n",
    "print(\" - anime_year_stats.csv (yearly stats)\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b2a5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dd2ee2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CORRELATION RESULTS ===\n",
      "Mean Score ↔ Market Value:  r = 0.3356, p = 5.0327e-03\n",
      "Anime Count ↔ Market Value: r = 0.7723, p = 0.0000e+00\n",
      "\n",
      "Saved results to correlation_results.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load datasets\n",
    "# -----------------------------\n",
    "anime = pd.read_csv(\"anime_year_stats.csv\")       # columns: year, anime_count, mean_score\n",
    "market = pd.read_csv(\"yearly_values.csv\")        # columns: Year, TotalValue_USD, etc.\n",
    "\n",
    "# Ensure column names match\n",
    "anime.rename(columns={\"year\": \"Year\"}, inplace=True)\n",
    "\n",
    "# Merge on Year\n",
    "df = anime.merge(market, on=\"Year\", how=\"inner\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Compute Pearson correlation manually\n",
    "# -----------------------------\n",
    "def pearson_corr(x, y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    numerator = np.sum((x - x_mean) * (y - y_mean))\n",
    "    denominator = np.sqrt(np.sum((x - x_mean)**2) * np.sum((y - y_mean)**2))\n",
    "    r = numerator / denominator\n",
    "    return r\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Compute two-tailed p-value from r\n",
    "# -----------------------------\n",
    "def pearson_p_value(r, n):\n",
    "    if n < 3:\n",
    "        return float('nan')\n",
    "    t_stat = r * math.sqrt((n - 2) / (1 - r**2))\n",
    "    # two-tailed p-value using normal approximation\n",
    "    p = 2 * (1 - 0.5 * (1 + math.erf(abs(t_stat) / math.sqrt(2))))\n",
    "    return p\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Compute correlations and p-values\n",
    "# -----------------------------\n",
    "n = len(df)\n",
    "\n",
    "# Mean score vs market value\n",
    "r_score = pearson_corr(df[\"mean_score\"], df[\"TotalValue_USD\"])\n",
    "p_score = pearson_p_value(r_score, n)\n",
    "\n",
    "# Anime count vs market value\n",
    "r_count = pearson_corr(df[\"anime_count\"], df[\"TotalValue_USD\"])\n",
    "p_count = pearson_p_value(r_count, n)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Print results\n",
    "# -----------------------------\n",
    "print(\"=== CORRELATION RESULTS ===\")\n",
    "print(f\"Mean Score ↔ Market Value:  r = {r_score:.4f}, p = {p_score:.4e}\")\n",
    "print(f\"Anime Count ↔ Market Value: r = {r_count:.4f}, p = {p_count:.4e}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Save results to file (UTF-8)\n",
    "# -----------------------------\n",
    "with open(\"correlation_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== CORRELATION RESULTS ===\\n\\n\")\n",
    "    f.write(f\"Mean Score ↔ Market Value\\n  r = {r_score:.4f}\\n  p = {p_score:.4e}\\n\\n\")\n",
    "    f.write(f\"Anime Count ↔ Market Value\\n  r = {r_count:.4f}\\n  p = {p_count:.4e}\\n\")\n",
    "\n",
    "print(\"\\nSaved results to correlation_results.txt\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python.venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
